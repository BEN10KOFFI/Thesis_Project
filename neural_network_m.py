# Unified Neural Network Script for Gabor and Standard Activations
import torch
import argparse
from src.get_setting_m import getSetting
import numpy as np
import tqdm

# Define Gabor layer where w and xi are generated by shared fully connected layers
class GaborLayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, sigma=1.0):
        super().__init__()
        self.generator = torch.nn.Sequential(
            torch.nn.Linear(in_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 2 * out_dim * in_dim)
        )
        self.sigma = sigma
        self.in_dim = in_dim
        self.out_dim = out_dim

    def forward(self, x):
        B = x.shape[0]
        params = self.generator(x)
        params = params.view(B, 2, self.out_dim, self.in_dim)
        w = params[:, 0, :, :]
        xi = params[:, 1, :, :]

        x_exp = x[:, None, :]
        envelope = torch.exp(-torch.sum((x_exp - xi) ** 2, dim=-1) / self.sigma ** 2)
        proj = torch.sum(x_exp * w, dim=-1)
        modulation = torch.exp(1j * proj)
        return envelope * modulation

class GaborNN(torch.nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.gabor = GaborLayer(in_dim, out_dim)
        self.output = torch.nn.Linear(out_dim, 1, dtype=torch.complex64)

    def forward(self, x):
        features = self.gabor(x)
        return self.output(features).real

class TwoLayerNN(torch.nn.Module):
    def __init__(self, activation, n_hidden, dim, bias, cplx):
        super().__init__()
        self.activation = activation
        self.layer1 = torch.nn.Linear(dim, n_hidden, bias=bias)
        self.layer2 = torch.nn.Linear(
            n_hidden, 1, bias=bias, dtype=torch.complex64 if cplx else torch.float
        )

    def forward(self, x):
        return self.layer2(self.activation(self.layer1(x))).real

# Argument parsing
parser = argparse.ArgumentParser(description="Unified Gabor and Standard NN baseline")
parser.add_argument("--setting", type=int, default=0)
parser.add_argument("--activation", type=str, default="Fourier")
parser.add_argument("--functions", type=bool, default=False)
inp = parser.parse_args()

synthetic_functions = inp.functions
setting = inp.setting

np.random.seed(1)
torch.manual_seed(1)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Activation setup
if inp.activation == "Fourier":
    act = lambda x: torch.exp(2j * torch.pi * x)
    bias = False
    cplx = True
elif inp.activation == "sigmoid":
    act = torch.nn.functional.sigmoid
    bias = True
    cplx = False
elif inp.activation == "relu":
    act = torch.nn.functional.relu
    bias = True
    cplx = False
elif inp.activation == "gabor":
    act = None  # not used, since GaborNN has internal logic
    bias = False
    cplx = True
else:
    raise ValueError("Unknown activation")

# Load data
points, fun_vals, test_points, test_vals = getSetting(synthetic_functions, setting)
n_train_points = int(0.9 * points.shape[0])
val_points = points[n_train_points:]
val_fun_vals = fun_vals[n_train_points:]
points = points[:n_train_points]
fun_vals = fun_vals[:n_train_points]
n_prior_points = max(1000, points.shape[0])
points_shift = torch.min(points, 0)[0]
points_scale = torch.max(points - points_shift, 0)[0]

# Î» search
lam_choices = [0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]
best_lam = None
best_mse = 1e8

for lam in lam_choices:
    model = GaborNN(points.shape[1], 100).to(device) if inp.activation == "gabor" else \
            TwoLayerNN(act, 100, points.shape[1], bias, cplx).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    iterations = 100000
    for i in (progress_bar := tqdm.tqdm(range(iterations))):
        preds = model(points).squeeze()
        mse = torch.mean((preds - fun_vals) ** 2)
        smoothness_prior = 0
        val_mse = -1
        if lam > 0:
            prior_points = torch.rand((n_prior_points, points.shape[1]), device=device) * points_scale + points_shift
            prior_points.requires_grad_(True)
            prior_preds = model(prior_points).squeeze()
            pred_diffs = torch.autograd.grad(torch.sum(prior_preds), prior_points, create_graph=True)[0]
            smoothness_prior = torch.mean(torch.sum(torch.abs(pred_diffs), -1))
        loss = mse + lam * smoothness_prior
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (i + 1) % 200 == 0:
            preds = model(val_points).squeeze()
            val_mse = torch.mean((preds - val_fun_vals) ** 2).item()
        progress_bar.set_description(
            "Loss: {0:.2E}, MSE: {1:.2E}, Validation MSE {2:.2E}".format(loss.item(), mse.item(), val_mse)
        )

    preds = model(val_points).squeeze()
    val_mse = torch.mean((preds - val_fun_vals) ** 2).item()
    if val_mse < best_mse:
        best_mse = val_mse
        best_lam = lam

# Final trials
n_trials = 5
nn_mses = []

for trial in range(n_trials):
    points, fun_vals, test_points, test_vals = getSetting(synthetic_functions, setting)
    n_train_points = int(0.9 * points.shape[0])
    val_points = points[n_train_points:]
    val_fun_vals = fun_vals[n_train_points:]
    points = points[:n_train_points]
    fun_vals = fun_vals[:n_train_points]

    model = GaborNN(points.shape[1], 100).to(device) if inp.activation == "gabor" else \
            TwoLayerNN(act, 100, points.shape[1], bias, cplx).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    iterations = 100000
    for i in (progress_bar := tqdm.tqdm(range(iterations))):
        preds = model(points).squeeze()
        mse = torch.mean((preds - fun_vals) ** 2)
        smoothness_prior = 0
        if best_lam > 0:
            prior_points = torch.rand((n_prior_points, points.shape[1]), device=device) * points_scale + points_shift
            prior_points.requires_grad_(True)
            prior_preds = model(prior_points).squeeze()
            pred_diffs = torch.autograd.grad(torch.sum(prior_preds), prior_points, create_graph=True)[0]
            smoothness_prior = torch.mean(torch.sum(torch.abs(pred_diffs), -1))
        loss = mse + best_lam * smoothness_prior
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (i + 1) % 200 == 0:
            preds = model(val_points).squeeze()
            val_mse = torch.mean((preds - val_fun_vals) ** 2).item()
        progress_bar.set_description(
            "Loss: {0:.2E}, MSE: {1:.2E}, Validation MSE {2:.2E}".format(loss.item(), mse.item(), val_mse)
        )

    with torch.no_grad():
        preds = model(test_points).squeeze()
        nn_mse = torch.mean((preds - test_vals) ** 2)
    print(nn_mse)
    nn_mses.append(nn_mse.item())

# Log results
title = "Setting {0}, lam {1:.2E}, functions {2}, activation {3}\n".format(setting, best_lam, synthetic_functions, inp.activation)
write_string1 = "Avg MSE is: {0:.2E}\n".format(np.mean(nn_mses))
print(write_string1)
with open("./log_nn.txt", "a") as f:
    f.write(title)
    f.write(write_string1)
    f.write("\n\n")
